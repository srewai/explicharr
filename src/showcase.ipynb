{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i don't plan to make any more changes unless for bug fix.\n",
    "\n",
    "here are three models, saved in `trial/model`\n",
    "\n",
    "- `model_o` is the original transformer\n",
    "- `model_e` is the new model with softmax attention, `e` for exponential\n",
    "- `model_q` is the new model with squared attention, `q` for square\n",
    "\n",
    "i like `model_e` the best, but judge for yourself :D see `pred/diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_e import Transformer\n",
    "trial = 'e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import PointedIndex\n",
    "from util_io import encode, decode\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cap = 256\n",
    "ckpt = 34990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer.new().data(len_cap= len_cap)\n",
    "forcing = model.forcing(trainable= False) # the model in teacher forcing mode\n",
    "autoreg = model.autoreg(trainable= False) # the model in autoregressive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trial/model/e34990\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.InteractiveSession()\n",
    "saver.restore(sess, \"trial/model/{}{}\".format(trial, ckpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_src = PointedIndex(np.load(\"trial/data/index_src.npy\").item())\n",
    "idx_tgt = PointedIndex(np.load(\"trial/data/index_tgt.npy\").item())\n",
    "\n",
    "def auto(s, m= autoreg, idx_src= idx_src, idx_tgt= idx_tgt, len_cap= len_cap):\n",
    "    # encode the sentence as a numpy array\n",
    "    # it's automatically padded at the beginning and the end\n",
    "    src = np.array(encode(idx_src, s))\n",
    "    # reshape the array into a batch with one instance\n",
    "    src.shape = 1, -1\n",
    "    # fetch the prediction\n",
    "    # tgt needs to be fed the first step (the padding at the beginning)\n",
    "    # len_tgt is the maximum steps to unroll\n",
    "    pred = m.pred.eval({m.src: src, m.tgt: src[:,:1], m.len_tgt: len_cap})\n",
    "    # take the only instance from the batch\n",
    "    pred = pred[0]\n",
    "    # decode the prediction\n",
    "    return decode(idx_tgt, pred)\n",
    "\n",
    "# to run the forcing model, remember to feed a encoded target sentence\n",
    "# and DON'T feed len_tgt\n",
    "# cuz that's simply the length of the encoded target minus the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Repeat Repeat Repeat Repeat .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto(\"Repeat !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you wanna know more about the training process for your own experiments,\n",
    "see `src/train_e.py` among others.\n",
    "(they have no difference except for which model to load).\n",
    "i commented out the code for training.\n",
    "it has code for profiling the graph, but i also copied the result to `src/trial/graph`.\n",
    "\n",
    "to understand the implementation of the model, see `model_e.py` among others.\n",
    "(there are minor variations.  the `diff` command would be useful.)\n",
    "\n",
    "don't hesitate to ask me if you have questions about my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the attention tensors in the teacher forcing model are these:\n",
    "```\n",
    "encode_forcing/layer1/att/attention/Reshape_1:0\n",
    "encode_forcing/layer2/att/attention/Reshape_1:0\n",
    "decode_forcing/layer1/att/causal/Reshape_1:0\n",
    "decode_forcing/layer1/att/attention/Reshape_1:0\n",
    "decode_forcing/layer2/att/causal/Reshape_1:0\n",
    "decode_forcing/layer2/att/attention/Reshape_1:0\n",
    "```\n",
    "\n",
    "the attention tensors in the autoregressive model are these:\n",
    "```\n",
    "encode_autoreg/layer1/att/attention/Reshape_1:0\n",
    "encode_autoreg/layer2/att/attention/Reshape_1:0\n",
    "decode_autoreg/autoreg/layer1/att/causal/Reshape_1:0\n",
    "decode_autoreg/autoreg/layer1/att/attention/Reshape_1:0\n",
    "decode_autoreg/autoreg/layer2/att/causal/Reshape_1:0\n",
    "decode_autoreg/autoreg/layer2/att/attention/Reshape_1:0\n",
    "```\n",
    "\n",
    "however tensorflow forbids the decoder attentions to be fetched, cuz they are inside of a loop.\n",
    "if you want these weights, you can run the forcing model repeatedly, feeding its previous predictions back each time, and accumulate the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
